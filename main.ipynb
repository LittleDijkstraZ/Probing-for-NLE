{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Explanations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import ast  # for converting embeddings saved as strings back to arrays\n",
    "import openai  \n",
    "import pandas as pd\n",
    "import tiktoken  # for counting tokens\n",
    "from scipy import spatial  # for calculating vector similarities for search\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "# GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "GPT_MODEL = \"gpt-4\" # According to OpenAI, GPT-4 is more responsive to system messages, whereas 3.5 would rely on more of the user input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example nle question\n",
    "premise = \"Which of the following should you not bring a fox in\"\n",
    "choices = ['hen house', 'England', 'elementary school']\n",
    "query = f\"\"\"Question: {premise}:\n",
    "Choices: {\"  \".join([f\"{i+1}.{choices[i]}\" for i in range(len(choices))])}\"\"\"\n",
    "\n",
    "sys_msg = \"\"\"\n",
    "Make a selection and explain your reasoning.\n",
    "Reply in the following format: the first line is contains only the choice number,\n",
    "and the second line is the explanation.\n",
    "\"\"\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': sys_msg},\n",
    "        {'role': 'user', 'content': query},\n",
    "    ],\n",
    "    model=GPT_MODEL,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "model_response = response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'Bringing a fox into a hen house would be disastrous, as foxes are natural predators of chickens and would likely cause harm or death to the hens.')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred, model_nle = model_response.split('\\n')\n",
    "y_pred = int(y_pred)\n",
    "y_pred, model_nle "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain NLE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')\n",
    "roberta.eval()  # disable dropout (or leave in train mode to finetune)\n",
    "roberta.to('cuda')\n",
    "tok_model_nle = roberta.encode(model_nle)\n",
    "tok_choices = [roberta.encode(ch) for ch in choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the tokens using the last layer feature of the model\n",
    "emb_model_nle = roberta.extract_features(tok_model_nle)\n",
    "tok_choices = [roberta.extract_features(ch) for ch in tok_choices]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate SHAP Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "X100 = shap.utils.sample(X, 100)\n",
    "shap.partial_dependence_plot(\n",
    "    your_feature_name, your_function_to_get_y_pred, X100, ice=False,\n",
    "    model_expected_value=True, feature_expected_value=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(your_function_to_get_y_pred, X100)\n",
    "shap_values = explainer(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
